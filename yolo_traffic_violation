# import the necessary packages
import numpy as np
import imutils
import time
import os
import cv2
from input_retrieval import *

# All these classes will be counted as 'vehicles'
list_of_vehicles = ["Mobil", "Sepeda Motor", "Bis", "Truk"]
inputWidth, inputHeight = 416, 416

# Parse command line arguments and extract the values required
LABELS, weightsPath, configPath, inputVideoPath, outputVideoPath, \
    preDefinedConfidence, preDefinedThreshold, USE_GPU = parseCommandLineArguments()

# Initialize a list of colors to represent each possible class label
np.random.seed(42)
COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")

# Define the detection zone (top-left and bottom-right coordinates)
zone1_top_left = (538, 140)  # Example coordinates
zone1_bottom_right = (811, 263)  # Example coordinates

# PURPOSE: Check if the center of the bounding box is inside the detection zone
# PARAMETERS: Bounding box coordinates and detection zone boundaries
# RETURN: True if inside the zone, otherwise False
def is_inside_detection_zone(x, y, top_left, bottom_right):
    if top_left and bottom_right:
        return top_left[0] <= x <= bottom_right[0] and top_left[1] <= y <= bottom_right[1]
    return False

def count_vehicles_in_zone(idxs, boxes, classIDs, zone_top_left, zone_bottom_right):
    message = "Tidak ada pelanggaran lalu lintas"

    if len(idxs) > 0:
        for i in idxs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])
            centerX, centerY = x + w // 2, y + h // 2

            if is_inside_detection_zone(centerX, centerY, zone_top_left, zone_bottom_right):
                    message = "Pelangaran Lalu lintas di deteksi!"

    return message

# Global variables to store the coordinates of the detection zone
zone_top_left = None
zone_bottom_right = None
drawing = False  # Flag to indicate whether we are currently drawing the rectangle

# Mouse callback function to capture the coordinates
def set_detection_zone(event, x, y, flags, param):
    global zone_top_left, zone_bottom_right, drawing

    # Record the starting point (top-left) on left mouse button down
    if event == cv2.EVENT_LBUTTONDOWN:
        zone_top_left = (x, y)
        drawing = True

    # Update the ending point (bottom-right) as the mouse moves
    elif event == cv2.EVENT_MOUSEMOVE:
        if drawing:
            zone_bottom_right = (x, y)

    # Finalize the rectangle on left mouse button up
    elif event == cv2.EVENT_LBUTTONUP:
        zone_bottom_right = (x, y)
        drawing = False
        print(f"Detection zone set: Top-left {zone_top_left}, Bottom-right {zone_bottom_right}")

# Function to draw the detection zone on the frame as it's being set
def draw_detection_zone(frame):
    if zone_top_left and zone_bottom_right:
        cv2.rectangle(frame, zone_top_left, zone_bottom_right, (255, 0, 0), 2)

# PURPOSE: Displays the FPS of the detected video
# PARAMETERS: Start time of the frame, number of frames within the same second
# RETURN: New start time, new number of frames 
def displayFPS(start_time, num_frames):
    current_time = int(time.time())
    if current_time > start_time:
        print("FPS:", num_frames)
        num_frames = 0
        start_time = current_time
    return start_time, num_frames

# PURPOSE: Draw all the detection boxes with a label and a green dot at the center
# PARAMETERS: Indexes of valid detections, bounding boxes, class IDs, confidences, frame
# RETURN: N/A
def drawDetectionBoxes(idxs, boxes, classIDs, confidences, frame):
    if len(idxs) > 0:
        for i in idxs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            color = [int(c) for c in COLORS[classIDs[i]]]
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            text = "{}: {:.4f}".format(LABELS[classIDs[i]], confidences[i])
            cv2.putText(frame, text, (x, y - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            cv2.circle(frame, (x + (w // 2), y + (h // 2)), 2, (0, 255, 0), thickness=2)

# PURPOSE: Count vehicles in the current frame by type
# PARAMETERS: Indexes of valid detections, bounding boxes, class IDs
# RETURN: A dictionary with vehicle types and their counts in the current frame
def count_vehicles_per_frame(idxs, boxes, classIDs):
    vehicle_count_dict = {vehicle: 0 for vehicle in list_of_vehicles}

    if len(idxs) > 0:
        for i in idxs.flatten():
            label = LABELS[classIDs[i]]
            if label in list_of_vehicles:
                vehicle_count_dict[label] += 1

            # Add Manusia count to Sepeda Motor
            if label == "Manusia":
                vehicle_count_dict["Sepeda Motor"] += 1

    return vehicle_count_dict

# PURPOSE: Display vehicle count per type on the frame
# PARAMETERS: Frame on which the counts are displayed, the count of each vehicle type
# RETURN: N/A
def displayVehicleCountPerType(frame, vehicle_count_dict):
    y_offset = 100
    for vehicle, count in vehicle_count_dict.items():
        text = f'{vehicle}: {count}'
        cv2.putText(frame, text, (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX,
                    0.6, (0, 255, 0), 2)
        y_offset += 20

# PURPOSE: Initializing the video writer with the output video path and the same number of fps, width and height as the source video 
# PARAMETERS: Width of the source video, Height of the source video, the video stream
# RETURN: The initialized video writer
def initializeVideoWriter(video_width, video_height, videoStream):
    sourceVideofps = videoStream.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*"MJPG")
    return cv2.VideoWriter(outputVideoPath, fourcc, sourceVideofps,
                           (video_width, video_height), True)

# load our YOLO object detector trained on COCO dataset (80 classes) and determine only the *output* layer names that we need from YOLO
print("[INFO] loading YOLO from disk...")
net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

# Using GPU if flag is passed
if USE_GPU:
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)

# Get the output layer names in the YOLO model
ln = net.getLayerNames()

# The following line needs to be modified to handle the scalar case
try:
    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]
except IndexError:
    ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]

# initialize the video stream, pointer to output video file, and
# frame dimensions
videoStream = cv2.VideoCapture(inputVideoPath)
if not videoStream.isOpened():
    print(f"Error: Could not open video {inputVideoPath}")
    exit()

video_width = int(videoStream.get(cv2.CAP_PROP_FRAME_WIDTH))
video_height = int(videoStream.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Set up mouse callback for the window
cv2.namedWindow("Frame")
cv2.setMouseCallback("Frame", set_detection_zone)

# Initialization
num_frames, vehicle_count = 0, 0
writer = initializeVideoWriter(video_width, video_height, videoStream)
start_time = int(time.time())

# loop over frames from the video file stream
while True:
    # read the next frame from the file
    (grabbed, frame) = videoStream.read()

    # if the frame was not grabbed, then we have reached the end of the stream
    if not grabbed:
        break

    # construct a blob from the input frame and then perform a forward pass of
    # the YOLO object detector, giving us our bounding boxes and associated probabilities
    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (inputWidth, inputHeight), swapRB=True, crop=False)
    net.setInput(blob)
    start = time.time()
    layerOutputs = net.forward(ln)
    end = time.time()

    # initialize our lists of detected bounding boxes, confidences, and class IDs, respectively
    boxes, confidences, classIDs = [], [], []

    # loop over each of the layer outputs
    for output in layerOutputs:
        # loop over each of the detections
        for detection in output:
            # extract the class ID and confidence (i.e., probability) of the current object detection
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            # filter out weak predictions by ensuring the detected probability is greater than the pre-defined confidence
            if confidence > preDefinedConfidence:
                # scale the bounding box coordinates back relative to the
                # size of the image, keeping in mind that YOLO actually
                # returns the center (x, y)-coordinates of the bounding box
                # followed by the boxes' width and height
                box = detection[0:4] * np.array([video_width, video_height, video_width, video_height])
                (centerX, centerY, width, height) = box.astype("int")

                # use the center (x, y)-coordinates to derive the top and left corner of the bounding box
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                confidences.append(float(confidence))
                classIDs.append(classID)

    # apply non-maxima suppression to suppress weak, overlapping bounding boxes
    idxs = cv2.dnn.NMSBoxes(boxes, confidences, preDefinedConfidence, preDefinedThreshold)

    # Draw the detection zone if it is set
    draw_detection_zone(frame)

    # Ensure the boxes are drawn for all valid detections
    drawDetectionBoxes(idxs, boxes, classIDs, confidences, frame)

    # Update the vehicle counts per type for each frame
    vehicle_count_dict = count_vehicles_per_frame(idxs, boxes, classIDs)
    # Display the vehicle counts per type on the frame
    displayVehicleCountPerType(frame, vehicle_count_dict)

    # Display FPS at the top-left corner of the frame
    num_frames += 1
    start_time, num_frames = displayFPS(start_time, num_frames)

        # display total detected vehicles in the current frame
    total_vehicles = sum(vehicle_count_dict.values())
    cv2.putText(
        frame,
        f'Total Vehicles: {total_vehicles}',
        (20, 20),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.8,
        (0, 255, 0),
        2,
        cv2.LINE_AA,
    )

    # Call the function to check for vehicles in the detection zone and get the message
    message = count_vehicles_in_zone(idxs, boxes, classIDs, zone_top_left, zone_bottom_right)

    # Display the zone vehicle count message on the frame
    cv2.putText(
        frame,
        f'{message}',
        (20, 50),
        cv2.FONT_HERSHEY_SIMPLEX,
        0.8,
        (0, 255, 0),
        2,
        cv2.LINE_AA,
    )

    cv2.rectangle(frame, zone1_top_left, zone1_bottom_right, (255, 0, 0), 2)

    # write the output frame to disk
    writer.write(frame)

    # Display the current frame to the screen
    cv2.imshow("Frame", frame)
    key = cv2.waitKey(1) & 0xFF

    # if the `q` key was pressed, break from the loop
    if key == ord("q"):
        break

# release the file pointers
print("[INFO] cleaning up...")
writer.release()
videoStream.release()
